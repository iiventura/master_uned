{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Práctica 1: Irene J. Ventura Farias\n",
    "\n",
    "##  Ejercicio1.3: Mejorando el país con mejores clientes.\n",
    "\n",
    "Devuelve el numero de clientes valorados con \"bueno\" por país.\n",
    "\n",
    "### Diseño MapReduce\n",
    "#### ¿Cuántos pasos MapReduce son necesarios?\n",
    "Para la solución de este problema  se van a realizar tres pasos: \n",
    "- Paso 1: un mapper y un reduce (Reduce_Count_Bueno).\n",
    "- Paso 2: un reducer (Reducer_Max_Country)\n",
    "#### ¿Qué hace cada función de cada paso?\n",
    "- _Mapper_: extrae los datos más relevantes de la información que recibimos\n",
    "    - Caso Paises: se obtiene el nombre completo del país.\n",
    "    - Caso Clientes: solo considera los clientes que califican 'bueno' a un país, contabilizando al cliente como 1 voto.\n",
    "- _Reducer_Count_Bueno: Obtiene el nombre completo y contabiliza el total de votos de cada país\n",
    "- _Reducer_Max_Country: Obtiene el máximo país con mayor votos\n",
    "#### ¿Qué datos se pasan de una función a la siguiente?\n",
    "- _Mapper_: \n",
    "    - Caso Paises: Sigla_País, [Símbolo='A', Nombre Completo del País]\n",
    "    - Caso Clientes: Sigla_País, [Símbolo='B', 1]\n",
    "- _Reducer_Count_Bueno: None , [contador_clientes, [Nombre Completo del País]]\n",
    "- _Reducer_Max_Country:  [contador_clientes, [Nombre Completo del País]] siendo el contador como el máximo general"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir -p mrjob/join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"/media/notebooks/mrjob/join\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/notebooks/mrjob/join\r\n"
     ]
    }
   ],
   "source": [
    "! pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Los ficheros countries.csv y clients.csv deben estar descargados en la carpeta /media/notebooks/mrjob/join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name,Code\n",
      "Afghanistan,AF\n",
      "Åland Islands,AX\n",
      "Albania,AL\n",
      "Algeria,DZ\n",
      "American Samoa,AS\n",
      "Andorra,AD\n",
      "Angola,AO\n",
      "Anguilla,AI\n",
      "Antarctica,AQ\n",
      "Antigua and Barbuda,AG\n",
      "Argentina,AR\n",
      "Armenia,AM\n",
      "Aruba,AW\n",
      "Australia,AU\n",
      "Austria,AT\n",
      "Azerbaijan,AZ\n",
      "Bahamas,BS\n",
      "Bahrain,BH\n",
      "Bangladesh,BD\n",
      "Barbados,BB\n",
      "Belarus,BY\n",
      "Belgium,BE\n",
      "Belize,BZ\n",
      "Benin,BJ\n",
      "Bermuda,BM\n",
      "Bhutan,BT\n",
      "\"Bolivia, Plurinational State of\",BO\n",
      "\"Bonaire, Sint Eustatius and Saba\",BQ\n",
      "Bosnia and Herzegovina,BA\n",
      "Botswana,BW\n",
      "Bouvet Island,BV\n",
      "Brazil,BR\n",
      "British Indian Ocean Territory,IO\n",
      "Brunei Darussalam,BN\n",
      "Bulgaria,BG\n",
      "Burkina Faso,BF\n",
      "Burundi,BI\n",
      "Cambodia,KH\n",
      "Cameroon,CM\n",
      "Canada,CA\n",
      "Cape Verde,CV\n",
      "Cayman Islands,KY\n",
      "Central African Republic,CF\n",
      "Chad,TD\n",
      "Chile,CL\n",
      "China,CN\n",
      "Christmas Island,CX\n",
      "Cocos (Keeling) Islands,CC\n",
      "Colombia,CO\n",
      "Comoros,KM\n",
      "Congo,CG\n",
      "\"Congo, the Democratic Republic of the\",CD\n",
      "Cook Islands,CK\n",
      "Costa Rica,CR\n",
      "Côte d'Ivoire,CI\n",
      "Croatia,HR\n",
      "Cuba,CU\n",
      "Curaçao,CW\n",
      "Cyprus,CY\n",
      "Czech Republic,CZ\n",
      "Denmark,DK\n",
      "Djibouti,DJ\n",
      "Dominica,DM\n",
      "Dominican Republic,DO\n",
      "Ecuador,EC\n",
      "Egypt,EG\n",
      "El Salvador,SV\n",
      "Equatorial Guinea,GQ\n",
      "Eritrea,ER\n",
      "Estonia,EE\n",
      "Ethiopia,ET\n",
      "Falkland Islands (Malvinas),FK\n",
      "Faroe Islands,FO\n",
      "Fiji,FJ\n",
      "Finland,FI\n",
      "France,FR\n",
      "French Guiana,GF\n",
      "French Polynesia,PF\n",
      "French Southern Territories,TF\n",
      "Gabon,GA\n",
      "Gambia,GM\n",
      "Georgia,GE\n",
      "Germany,DE\n",
      "Ghana,GH\n",
      "Gibraltar,GI\n",
      "Greece,GR\n",
      "Greenland,GL\n",
      "Grenada,GD\n",
      "Guadeloupe,GP\n",
      "Guam,GU\n",
      "Guatemala,GT\n",
      "Guernsey,GG\n",
      "Guinea,GN\n",
      "Guinea-Bissau,GW\n",
      "Guyana,GY\n",
      "Haiti,HT\n",
      "Heard Island and McDonald Islands,HM\n",
      "Holy See (Vatican City State),VA\n",
      "Honduras,HN\n",
      "Hong Kong,HK\n",
      "Hungary,HU\n",
      "Iceland,IS\n",
      "India,IN\n",
      "Indonesia,ID\n",
      "\"Iran, Islamic Republic of\",IR\n",
      "Iraq,IQ\n",
      "Ireland,IE\n",
      "Isle of Man,IM\n",
      "Israel,IL\n",
      "Italy,IT\n",
      "Jamaica,JM\n",
      "Japan,JP\n",
      "Jersey,JE\n",
      "Jordan,JO\n",
      "Kazakhstan,KZ\n",
      "Kenya,KE\n",
      "Kiribati,KI\n",
      "\"Korea, Democratic People's Republic of\",KP\n",
      "\"Korea, Republic of\",KR\n",
      "Kuwait,KW\n",
      "Kyrgyzstan,KG\n",
      "Lao People's Democratic Republic,LA\n",
      "Latvia,LV\n",
      "Lebanon,LB\n",
      "Lesotho,LS\n",
      "Liberia,LR\n",
      "Libya,LY\n",
      "Liechtenstein,LI\n",
      "Lithuania,LT\n",
      "Luxembourg,LU\n",
      "Macao,MO\n",
      "\"Macedonia, the Former Yugoslav Republic of\",MK\n",
      "Madagascar,MG\n",
      "Malawi,MW\n",
      "Malaysia,MY\n",
      "Maldives,MV\n",
      "Mali,ML\n",
      "Malta,MT\n",
      "Marshall Islands,MH\n",
      "Martinique,MQ\n",
      "Mauritania,MR\n",
      "Mauritius,MU\n",
      "Mayotte,YT\n",
      "Mexico,MX\n",
      "\"Micronesia, Federated States of\",FM\n",
      "\"Moldova, Republic of\",MD\n",
      "Monaco,MC\n",
      "Mongolia,MN\n",
      "Montenegro,ME\n",
      "Montserrat,MS\n",
      "Morocco,MA\n",
      "Mozambique,MZ\n",
      "Myanmar,MM\n",
      "Namibia,NA\n",
      "Nauru,NR\n",
      "Nepal,NP\n",
      "Netherlands,NL\n",
      "New Caledonia,NC\n",
      "New Zealand,NZ\n",
      "Nicaragua,NI\n",
      "Niger,NE\n",
      "Nigeria,NG\n",
      "Niue,NU\n",
      "Norfolk Island,NF\n",
      "Northern Mariana Islands,MP\n",
      "Norway,NO\n",
      "Oman,OM\n",
      "Pakistan,PK\n",
      "Palau,PW\n",
      "\"Palestine, State of\",PS\n",
      "Panama,PA\n",
      "Papua New Guinea,PG\n",
      "Paraguay,PY\n",
      "Peru,PE\n",
      "Philippines,PH\n",
      "Pitcairn,PN\n",
      "Poland,PL\n",
      "Portugal,PT\n",
      "Puerto Rico,PR\n",
      "Qatar,QA\n",
      "Réunion,RE\n",
      "Romania,RO\n",
      "Russian Federation,RU\n",
      "Rwanda,RW\n",
      "Saint Barthélemy,BL\n",
      "\"Saint Helena, Ascension and Tristan da Cunha\",SH\n",
      "Saint Kitts and Nevis,KN\n",
      "Saint Lucia,LC\n",
      "Saint Martin (French part),MF\n",
      "Saint Pierre and Miquelon,PM\n",
      "Saint Vincent and the Grenadines,VC\n",
      "Samoa,WS\n",
      "San Marino,SM\n",
      "Sao Tome and Principe,ST\n",
      "Saudi Arabia,SA\n",
      "Senegal,SN\n",
      "Serbia,RS\n",
      "Seychelles,SC\n",
      "Sierra Leone,SL\n",
      "Singapore,SG\n",
      "Sint Maarten (Dutch part),SX\n",
      "Slovakia,SK\n",
      "Slovenia,SI\n",
      "Solomon Islands,SB\n",
      "Somalia,SO\n",
      "South Africa,ZA\n",
      "South Georgia and the South Sandwich Islands,GS\n",
      "South Sudan,SS\n",
      "Spain,ES\n",
      "Sri Lanka,LK\n",
      "Sudan,SD\n",
      "Suriname,SR\n",
      "Svalbard and Jan Mayen,SJ\n",
      "Swaziland,SZ\n",
      "Sweden,SE\n",
      "Switzerland,CH\n",
      "Syrian Arab Republic,SY\n",
      "\"Taiwan, Province of China\",TW\n",
      "Tajikistan,TJ\n",
      "\"Tanzania, United Republic of\",TZ\n",
      "Thailand,TH\n",
      "Timor-Leste,TL\n",
      "Togo,TG\n",
      "Tokelau,TK\n",
      "Tonga,TO\n",
      "Trinidad and Tobago,TT\n",
      "Tunisia,TN\n",
      "Turkey,TR\n",
      "Turkmenistan,TM\n",
      "Turks and Caicos Islands,TC\n",
      "Tuvalu,TV\n",
      "Uganda,UG\n",
      "Ukraine,UA\n",
      "United Arab Emirates,AE\n",
      "United Kingdom,GB\n",
      "United States,US\n",
      "United States Minor Outlying Islands,UM\n",
      "Uruguay,UY\n",
      "Uzbekistan,UZ\n",
      "Vanuatu,VU\n",
      "\"Venezuela, Bolivarian Republic of\",VE\n",
      "Viet Nam,VN\n",
      "\"Virgin Islands, British\",VG\n",
      "\"Virgin Islands, U.S.\",VI\n",
      "Wallis and Futuna,WF\n",
      "Western Sahara,EH\n",
      "Yemen,YE\n",
      "Zambia,ZM\n",
      "Zimbabwe,ZW\n"
     ]
    }
   ],
   "source": [
    "cat countries.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bertram Pearcy  ,bueno,SO\n",
      "Steven Ulman  ,regular,ZA\n",
      "Enid Follansbee  ,malo,GS\n",
      "Candie Jacko  ,malo,SS\n",
      "Alana Zufelt  ,regular,ES\n",
      "Craig Pinkett  ,malo,LK\n",
      "Carson Levey  ,bueno,GU\n",
      "Reanna Calabrese  ,regular,GT\n",
      "Elliott Kosak  ,malo,GG\n",
      "Yuette Steinman  ,bueno,GN\n",
      "Grisel Wines  ,regular,GW\n",
      "Kathryne Dieguez  ,regular,AE\n",
      "Donna Raabe  ,malo,GB\n",
      "Norine Mundt  ,bueno,US\n",
      "Brittaney Amaro  ,bueno,ES\n",
      "Penni Husted  ,bueno,ES\n",
      "Delmer Semon  ,malo,IT\n",
      "Lennie Dunkerson  ,bueno,CA\n",
      "Mayra Bobb  ,regular,IT\n",
      "Altagracia Merced  ,regular,CA\n",
      "Verda Belgrave  ,malo,GB\n",
      "Jonnie Urban  ,malo,US\n",
      "Chung Frankum  ,malo,ES\n",
      "Vincenzo Samples  ,regular,TT\n",
      "Dominick Barkan  ,bueno,GU\n",
      "Carisa Ellingwood  ,bueno,TR\n",
      "Garret Wess  ,regular,TM\n",
      "Zoraida Muise  ,bueno,GU\n",
      "Samantha Cusson  ,bueno,PT\n",
      "Jenine Greenburg  ,regular,PR\n",
      "Geri Paddock  ,bueno,QA\n",
      "Antonia Klosterman  ,regular,RE\n",
      "Moriah Galey  ,malo,RO\n",
      "Nyla Eckard  ,malo,GB\n",
      "Arlean Harries  ,malo,US\n",
      "Kenyatta Lippold  ,malo,ES\n",
      "Samuel Knipe  ,malo,MV\n",
      "Jamison Starner  ,malo,ML\n",
      "Joel Blye  ,regular,MT\n",
      "Adela Jaimes  ,regular,GB\n",
      "Isis Sorrells  ,regular,US\n",
      "Chester Abdul  ,regular,ES\n",
      "Manda Wingate  ,regular,SI\n",
      "Anna Rappold  ,regular,SB\n",
      "Albina Lamore  ,malo,SO\n",
      "Carolyn Machado  ,bueno,ZA\n",
      "Jeni Espinoza  ,bueno,GS\n",
      "Charisse Salzman  ,bueno,SS\n",
      "Dorla Silber  ,bueno,ES\n",
      "Lilli Bryson  ,malo,LK\n"
     ]
    }
   ],
   "source": [
    "cat clients.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mrjob-ejercicio.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mrjob-ejercicio.py\n",
    "import sys, os, re\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "\n",
    "class MRJoin(MRJob):\n",
    "\n",
    "  # Realiza la ordenacion secundaria\n",
    "    SORT_VALUES = True\n",
    "\n",
    "    \"\"\"El mapper se encarga de extraer la siglas del pais como clave. Como valor para el caso de los paises\n",
    "    nos quedamos con su nombre completo y en cuanto a los clientes filtramos los que hayan calificado\n",
    "    como 'bueno' a un paísy asignamos 1 como contador\"\"\"\n",
    "    def mapper(self, _, line):\n",
    "        splits = line.rstrip(\"\\n\").split(\",\")\n",
    "\n",
    "        if len(splits) == 2: # datos de paises\n",
    "            symbol = 'A' # ordenamos los paises antes que los datos de personas\n",
    "            country2digit = splits[1]\n",
    "            yield country2digit, [symbol, splits[0]]\n",
    "                \n",
    "        else: #  datos de personas\n",
    "            if(splits[1]=='bueno'):\n",
    "                symbol = 'B'\n",
    "                country2digit = splits[2]\n",
    "                yield country2digit, [symbol, 1] #contabiliza como 1 voto\n",
    "                \n",
    "    \n",
    "    \"\"\"El reducer obtiene una lista de clave (siglas del país) y el valor es una lista de listas que contiene\n",
    "    el nombre completo del país (A) y cuantas veces han votado como bueno (B)\"\"\"\n",
    "    def reducer_count_bueno(self, key, values):\n",
    "        \n",
    "        for value in values:\n",
    "            if value[0] == 'A': #dato del país\n",
    "                count = 0 # iniciamos el contador\n",
    "                country_name = value[1] #asigamos el nombre completo\n",
    "                \n",
    "            if value[0] == 'B':\n",
    "                count +=1\n",
    "                \n",
    "        # Comprobamos que el pais tenga al menos un  voto. Devolvemos el numero de votos y el nombre completo del país\n",
    "        if count >0:\n",
    "            yield None,[count, [country_name]] \n",
    "            \n",
    "    \"\"\"Buscamos todos los paises con mayor votos calificados como 'bueno' \"\"\"        \n",
    "    def reducer_max_countries(self, key, values):\n",
    "        max_country = 0\n",
    "        \n",
    "        for c in reversed(list(values)): #recorre la lista de mayor a menor\n",
    "            # si el actual es mayor o igual al ultimo mayor se actualiza\n",
    "            if(c[0]>=max_country):\n",
    "                max_country = c[0]\n",
    "                yield c\n",
    "            \n",
    "            else: break\n",
    "        \n",
    "    \"\"\"Usamos el método step para indicar el orden de ejecución del mapper y los dos reducers\"\"\"    \n",
    "    def steps(self):\n",
    "        return [MRStep(mapper=self.mapper,                \n",
    "                       reducer=self.reducer_count_bueno),\n",
    "                MRStep(reducer=self.reducer_max_countries)]\n",
    "            \n",
    "                \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MRJoin.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejecutamos primero en local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for inline runner\n",
      "Creating temp directory /tmp/mrjob-ejercicio.root.20211114.122930.551166\n",
      "Running step 1 of 2...\n",
      "Running step 2 of 2...\n",
      "job output is in /tmp/mrjob-ejercicio.root.20211114.122930.551166/output\n",
      "Streaming final output from /tmp/mrjob-ejercicio.root.20211114.122930.551166/output...\n",
      "Removing temp directory /tmp/mrjob-ejercicio.root.20211114.122930.551166...\n"
     ]
    }
   ],
   "source": [
    "! python3 mrjob-ejercicio.py /media/notebooks/mrjob/join/countries.csv  \\\n",
    "/media/notebooks/mrjob/join/clients.csv > ouput3local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\t[\"Spain\"]\r\n",
      "3\t[\"Guam\"]\r\n"
     ]
    }
   ],
   "source": [
    "#Comprobamos que la salida coincida con lo solicitado en el ejercicio\n",
    "! tail ouput3local"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejecutamos en el Clúster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: `/tmp/mrjoin': File exists\n",
      "put: `/tmp/mrjoin/countries.csv': File exists\n",
      "put: `/tmp/mrjoin/clients.csv': File exists\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -mkdir /tmp/mrjoin #Creamos la ruta temporal para los ejercicios de MapReduce\n",
    "#Asignamos los fichero a la ruta temp/mrjoin en Hadoop\n",
    "! hdfs dfs -put /media/notebooks/mrjob/join/countries.csv  /tmp/mrjoin\n",
    "! hdfs dfs -put /media/notebooks/mrjob/join/clients.csv  /tmp/mrjoin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\n",
      "-rw-r--r--   3 root supergroup       1289 2021-11-14 12:55 /tmp/mrjoin/clients.csv\n",
      "-rw-r--r--   3 root supergroup       4120 2021-11-14 12:55 /tmp/mrjoin/countries.csv\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -ls  /tmp/mrjoin #Comprobamos que se haya copiado correctamente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /tmp/carpeta/mrjob-join-output/_SUCCESS\n",
      "Deleted /tmp/carpeta/mrjob-join-output/part-00000\n"
     ]
    }
   ],
   "source": [
    "#Reseteamos los ficheros y el directorio de salida\n",
    "! hdfs dfs -rm /tmp/carpeta/mrjob-join-output/*\n",
    "! hdfs dfs -rmdir /tmp/carpeta/mrjob-join-output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for hadoop runner\n",
      "Looking for hadoop binary in /app/hadoop-3.3.1/bin...\n",
      "Found hadoop binary: /app/hadoop-3.3.1/bin/hadoop\n",
      "Using Hadoop version 3.3.1\n",
      "Looking for Hadoop streaming jar in /app/hadoop-3.3.1...\n",
      "Found Hadoop streaming jar: /app/hadoop-3.3.1/share/hadoop/tools/lib/hadoop-streaming-3.3.1.jar\n",
      "Creating temp directory /tmp/mrjob-ejercicio.root.20211114.123240.488604\n",
      "uploading working dir files to hdfs:///user/root/tmp/mrjob/mrjob-ejercicio.root.20211114.123240.488604/files/wd...\n",
      "Copying other local files to hdfs:///user/root/tmp/mrjob/mrjob-ejercicio.root.20211114.123240.488604/files/\n",
      "Running step 1 of 2...\n",
      "  packageJobJar: [/tmp/hadoop-unjar592259833676980320/] [] /tmp/streamjob3771564844690051844.jar tmpDir=null\n",
      "  Connecting to ResourceManager at yarnmaster/172.18.0.4:8032\n",
      "  Connecting to ResourceManager at yarnmaster/172.18.0.4:8032\n",
      "  Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/root/.staging/job_1636883453227_0018\n",
      "  Total input files to process : 2\n",
      "  number of splits:3\n",
      "  Submitting tokens for job: job_1636883453227_0018\n",
      "  Executing with tokens: []\n",
      "  resource-types.xml not found\n",
      "  Unable to find 'resource-types.xml'.\n",
      "  Submitted application application_1636883453227_0018\n",
      "  The url to track the job: http://yarnmaster:8088/proxy/application_1636883453227_0018/\n",
      "  Running job: job_1636883453227_0018\n",
      "  Job job_1636883453227_0018 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1636883453227_0018 completed successfully\n",
      "  Output directory: hdfs:///user/root/tmp/mrjob/mrjob-ejercicio.root.20211114.123240.488604/step-output/0000\n",
      "Counters: 54\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=6825\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=307\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=6882\n",
      "\t\tFILE: Number of bytes written=1128561\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=7114\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\t\tHDFS: Number of bytes written=307\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=14\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=3\n",
      "\t\tLaunched map tasks=3\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=276229120\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=28498944\n",
      "\t\tTotal time spent by all map tasks (ms)=269755\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=269755\n",
      "\t\tTotal time spent by all reduce tasks (ms)=27831\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=27831\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=269755\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=27831\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=9360\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=2752\n",
      "\t\tInput split bytes=289\n",
      "\t\tMap input records=300\n",
      "\t\tMap output bytes=6376\n",
      "\t\tMap output materialized bytes=6894\n",
      "\t\tMap output records=250\n",
      "\t\tMerged Map outputs=3\n",
      "\t\tPeak Map Physical memory (bytes)=229462016\n",
      "\t\tPeak Map Virtual memory (bytes)=2485604352\n",
      "\t\tPeak Reduce Physical memory (bytes)=125726720\n",
      "\t\tPeak Reduce Virtual memory (bytes)=2490290176\n",
      "\t\tPhysical memory (bytes) snapshot=812621824\n",
      "\t\tReduce input groups=246\n",
      "\t\tReduce input records=250\n",
      "\t\tReduce output records=12\n",
      "\t\tReduce shuffle bytes=6894\n",
      "\t\tShuffled Maps =3\n",
      "\t\tSpilled Records=500\n",
      "\t\tTotal committed heap usage (bytes)=566505472\n",
      "\t\tVirtual memory (bytes) snapshot=9947103232\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Running step 2 of 2...\n",
      "  packageJobJar: [/tmp/hadoop-unjar6774905813993603985/] [] /tmp/streamjob101521952965780495.jar tmpDir=null\n",
      "  Connecting to ResourceManager at yarnmaster/172.18.0.4:8032\n",
      "  Connecting to ResourceManager at yarnmaster/172.18.0.4:8032\n",
      "  Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/root/.staging/job_1636883453227_0019\n",
      "  Total input files to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1636883453227_0019\n",
      "  Executing with tokens: []\n",
      "  resource-types.xml not found\n",
      "  Unable to find 'resource-types.xml'.\n",
      "  Submitted application application_1636883453227_0019\n",
      "  The url to track the job: http://yarnmaster:8088/proxy/application_1636883453227_0019/\n",
      "  Running job: job_1636883453227_0019\n",
      "  Job job_1636883453227_0019 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1636883453227_0019 completed successfully\n",
      "  Output directory: hdfs:///tmp/carpeta/mrjob-join-output\n",
      "Counters: 54\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=461\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=23\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=349\n",
      "\t\tFILE: Number of bytes written=836575\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=789\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\t\tHDFS: Number of bytes written=23\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=11\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=2\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=125788160\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=29896704\n",
      "\t\tTotal time spent by all map tasks (ms)=122840\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=122840\n",
      "\t\tTotal time spent by all reduce tasks (ms)=29196\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=29196\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=122840\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=29196\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=6910\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=1620\n",
      "\t\tInput split bytes=328\n",
      "\t\tMap input records=12\n",
      "\t\tMap output bytes=319\n",
      "\t\tMap output materialized bytes=355\n",
      "\t\tMap output records=12\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPeak Map Physical memory (bytes)=228708352\n",
      "\t\tPeak Map Virtual memory (bytes)=2485604352\n",
      "\t\tPeak Reduce Physical memory (bytes)=125263872\n",
      "\t\tPeak Reduce Virtual memory (bytes)=2490404864\n",
      "\t\tPhysical memory (bytes) snapshot=581885952\n",
      "\t\tReduce input groups=12\n",
      "\t\tReduce input records=12\n",
      "\t\tReduce output records=2\n",
      "\t\tReduce shuffle bytes=355\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=24\n",
      "\t\tTotal committed heap usage (bytes)=398663680\n",
      "\t\tVirtual memory (bytes) snapshot=7461613568\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "job output is in hdfs:///tmp/carpeta/mrjob-join-output\n",
      "Removing HDFS temp directory hdfs:///user/root/tmp/mrjob/mrjob-ejercicio.root.20211114.123240.488604...\n",
      "Removing temp directory /tmp/mrjob-ejercicio.root.20211114.123240.488604...\n"
     ]
    }
   ],
   "source": [
    "! python3 mrjob-ejercicio.py hdfs:///tmp/mrjoin/* -r hadoop --output-dir hdfs:///tmp/carpeta/mrjob-join-output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\r\n",
      "-rw-r--r--   3 root supergroup          0 2021-11-14 13:40 /tmp/carpeta/mrjob-join-output/_SUCCESS\r\n",
      "-rw-r--r--   3 root supergroup         23 2021-11-14 13:40 /tmp/carpeta/mrjob-join-output/part-00000\r\n"
     ]
    }
   ],
   "source": [
    "#Comprobamos si se ha procesado correctamente \n",
    "! hdfs dfs -ls  /tmp/carpeta/mrjob-join-output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\t[\"Spain\"]\r\n",
      "3\t[\"Guam\"]\r\n"
     ]
    }
   ],
   "source": [
    "#Mostramos el contenido del fichero de salida\n",
    "! hdfs dfs -tail /tmp/carpeta/mrjob-join-output/part-00000"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
